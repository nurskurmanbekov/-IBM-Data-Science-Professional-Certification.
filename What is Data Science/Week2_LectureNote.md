# Week 2
<b>Big Data Note:</b>

Definition:

Refers to large, diverse and dynamic data generated by people, tools and machines.
Technology is needed to process and analyze data to derive real-time insights.
V's of Big Data:

Velocity: Data accumulates fast with near or real-time streaming.
Volume: Increasing amount of data stored due to more data sources and scalable infrastructure.
Variety: Diverse types of data including structured and unstructured data.
Veracity: Quality and accuracy of data, must ensure consistency and completeness.
Value: Ability to turn data into value, may have medical or social benefits.
Examples:

Velocity: Hours of footage uploaded to YouTube every 60 seconds.
Volume: 2.5 quintillion bytes of data generated every day by digital devices.
Variety: Different types of data including text, pictures, film, sound, health data, and IoT.
Veracity: 80% of data is unstructured, need to categorize and analyze to ensure accuracy.
Value: Distributed computing tools such as Apache Spark, Hadoop to extract and process data for insights.
Benefits:

Connects organizations with customers and enriches services offered.
Data Scientists derive insights from Big Data using distributed computing power.
Remember:

Every interaction with a digital device generates data that may undergo big data analysis.

Big Data refers to the vast and diverse amount of data that is generated by people, machines, and tools. The key elements of Big Data include velocity, volume, variety, veracity, and value. Velocity refers to the speed at which data accumulates, volume is the scale of the data and its increase in amount, variety refers to the diversity of data, veracity is the quality and origin of data and its accuracy, and value refers to the ability to turn data into meaningful insights. To handle this massive data, new and innovative technology is required, such as Apache Spark, Hadoop, and others, that can process and analyze the data in real-time. The traditional way of processing data was to bring the data to the computer, but in a big data cluster, the data is sliced into pieces and distributed to multiple computers, which run the same program on their piece and send the results back. This approach, known as MapReduce, allows for processing large data sets and is widely used by major social media companies. Hadoop is an open-source software framework for storing and processing large data sets.

The data mining process consists of several stages: establishing data mining goals, selecting data, preprocessing data, transforming data, storing data, mining data, and evaluating mining results. In establishing goals, you must identify the key questions to be answered and determine the cost-benefit trade-off for the desired level of accuracy. Selecting data requires identifying the right kind of data needed for data mining at reasonable costs. Preprocessing data involves identifying irrelevant attributes and eliminating errors and missing data. Transforming data involves reducing the number of attributes and transforming variables into a more suitable format. Storing data involves ensuring the data is stored in a format that allows for efficient read/write privileges and security. Mining data involves analyzing the data using various methods such as data visualization, parametric, non-parametric methods, and machine-learning algorithms. Evaluating the results involves testing the predictive capabilities of the models and getting feedback from stakeholders to improve the process. This process is iterative, with improvements made over time based on the results and feedback.

The terms "big data," "data mining," "machine learning," "deep learning," "artificial neural networks," "artificial intelligence," and "data science" are all related to the field of analyzing and making decisions based on large amounts of data. Big data refers to massive, quickly generated and diverse data sets that traditional methods can't handle. Data mining is the process of searching and analyzing data to find patterns. Machine learning uses computer algorithms to analyze data and make decisions without explicit programming, while deep learning is a subset of machine learning that uses layered neural networks to simulate human decision-making. Artificial neural networks are collections of computing units inspired by biological neural networks, and AI refers to the ability of computers to solve problems and make intelligent decisions. Data science encompasses the entire process of extracting knowledge and insights from data, and can use AI techniques to achieve this. Recommender systems and predictive analytics are two major applications of machine learning, while fraud detection is an application in fintech. It's important for someone in data science to understand the meaning of these terms and the trade-offs between them.

Regression models are statistical techniques that are widely used in academic and professional fields to study the relationship between two or more variables. They are used to quantify the magnitude of the relationship between variables and to answer questions about the impact of one variable on another. For example, in the case of a study on residential real estate properties, regression models were used to determine the impact of the size of a housing unit, proximity to transport infrastructure, shopping centers, and downtown on the housing price. The real value added by the research was in quantifying the magnitude of these relationships, which confirmed what people already knew from their everyday experience but with more precision.